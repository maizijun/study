{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression,Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('c:/Users/qqmai/Desktop/mai.zjsh/sku_pred/luo.sz/appendata/y2018_feature_上海_春.csv',\n",
    "header=0,engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_select = [*map(lambda x:all(['huan' not in x,'mean' not in x,'Unnamed' not in x,\n",
    "'bin' not in x,'first' not in x,'last' not in x,'weight' not in x,'cumsum' not in x,\n",
    "'diff' not in x,'rank' not in x,'ratio' not in x])\n",
    ",x.columns)]\n",
    "x = x[x.columns[is_select]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['categ'] = x['product_code'].apply(lambda x:x[-2])\n",
    "x['season'] = x['product_code'].apply(lambda x:x[-3])\n",
    "x['color'] = x['product_code'].apply(lambda x:x[-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     product_code  date_week_four  qty_sum_one  qty_sum_two  qty_sum_three  \\\n0  B4CS2K1DDA1AM8             3.0          3.0          4.0            4.0   \n1  B4CS2K1DDU1AM8             3.0          5.0          2.0            9.0   \n2  B4DS2J1DDU1AM8             3.0         13.0          5.0            8.0   \n3  B4PS2X1DDS1AQ8             3.0          0.0          0.0            0.0   \n4  B4PS2X1DDU1AQ8             3.0          0.0          0.0            0.0   \n\n   qty_sum_four  discount_one  discount_two  discount_three  discount_four  \\\n0           5.0      0.740452      0.722191        0.763626       0.700111   \n1          28.0      0.741046      0.721913        0.730070       0.738320   \n2           7.0      0.709072      0.725130        0.706986       0.701326   \n3           0.0      0.000000      0.000000        0.690338       0.690338   \n4           0.0      0.000000      0.000000        0.767171       0.767171   \n\n   ...    ftr+store4  ftr+store5  ftr_qty1  ftr_qty2  ftr_qty3  ftr_qty4  \\\n0  ...          58.0   58.000000       0.0       8.0       5.0       4.0   \n1  ...          58.0   54.142857      14.0      15.0      16.0      15.0   \n2  ...          57.0   58.714286       1.0       1.0      11.0      11.0   \n3  ...          43.0   43.000000       0.0       0.0       2.0       1.0   \n4  ...          43.0   43.000000       0.0       0.0       1.0       2.0   \n\n   ftr_qty5  categ  season  color  \n0       4.0      M       A      A  \n1      12.0      M       A      U  \n2      17.0      M       A      U  \n3       2.0      Q       A      S  \n4       7.0      Q       A      U  \n\n[5 rows x 40 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_code</th>\n      <th>date_week_four</th>\n      <th>qty_sum_one</th>\n      <th>qty_sum_two</th>\n      <th>qty_sum_three</th>\n      <th>qty_sum_four</th>\n      <th>discount_one</th>\n      <th>discount_two</th>\n      <th>discount_three</th>\n      <th>discount_four</th>\n      <th>...</th>\n      <th>ftr+store4</th>\n      <th>ftr+store5</th>\n      <th>ftr_qty1</th>\n      <th>ftr_qty2</th>\n      <th>ftr_qty3</th>\n      <th>ftr_qty4</th>\n      <th>ftr_qty5</th>\n      <th>categ</th>\n      <th>season</th>\n      <th>color</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>B4CS2K1DDA1AM8</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>0.740452</td>\n      <td>0.722191</td>\n      <td>0.763626</td>\n      <td>0.700111</td>\n      <td>...</td>\n      <td>58.0</td>\n      <td>58.000000</td>\n      <td>0.0</td>\n      <td>8.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>M</td>\n      <td>A</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B4CS2K1DDU1AM8</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>9.0</td>\n      <td>28.0</td>\n      <td>0.741046</td>\n      <td>0.721913</td>\n      <td>0.730070</td>\n      <td>0.738320</td>\n      <td>...</td>\n      <td>58.0</td>\n      <td>54.142857</td>\n      <td>14.0</td>\n      <td>15.0</td>\n      <td>16.0</td>\n      <td>15.0</td>\n      <td>12.0</td>\n      <td>M</td>\n      <td>A</td>\n      <td>U</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>B4DS2J1DDU1AM8</td>\n      <td>3.0</td>\n      <td>13.0</td>\n      <td>5.0</td>\n      <td>8.0</td>\n      <td>7.0</td>\n      <td>0.709072</td>\n      <td>0.725130</td>\n      <td>0.706986</td>\n      <td>0.701326</td>\n      <td>...</td>\n      <td>57.0</td>\n      <td>58.714286</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>11.0</td>\n      <td>11.0</td>\n      <td>17.0</td>\n      <td>M</td>\n      <td>A</td>\n      <td>U</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B4PS2X1DDS1AQ8</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.690338</td>\n      <td>0.690338</td>\n      <td>...</td>\n      <td>43.0</td>\n      <td>43.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>Q</td>\n      <td>A</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>B4PS2X1DDU1AQ8</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.767171</td>\n      <td>0.767171</td>\n      <td>...</td>\n      <td>43.0</td>\n      <td>43.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>7.0</td>\n      <td>Q</td>\n      <td>A</td>\n      <td>U</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 40 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "df_color = pd.DataFrame(ohe.fit_transform(x[['color']]).toarray())\n",
    "df_color.columns = ['color_'+str(k) for k in df_color.columns]\n",
    "\n",
    "df_categ = pd.DataFrame(ohe.fit_transform(x[['categ']]).toarray())\n",
    "df_categ.columns = ['categ_'+str(k) for k in df_categ.columns]\n",
    "\n",
    "df_season = pd.DataFrame(ohe.fit_transform(x[['season']]).toarray())\n",
    "df_season.columns = ['season_'+str(k) for k in df_season.columns]\n",
    "\n",
    "x1,y1 = x.iloc[:,2:-14],x.iloc[:,-8:-3].sum(axis=1) \n",
    "x1 = pd.concat([x1,df_color,df_categ,df_season],axis=1)\n",
    "y1 = np.expand_dims(y1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(x1,y1,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((8194, 42), (3512, 42), (8194, 1), (3512, 1))"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "y_train = y_train.astype('float')\n",
    "y_test = y_test.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_field\n",
    "input_field = [0]*24+[1]*14+[2]*2+[3]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fea_index = tf.placeholder(tf.int32,shape=[None,None],name='feature_index')\n",
    "# fea_value = tf.placeholder(tf.int32,shape=[None,None],name='feature_value')\n",
    "# fea_label = tf.placeholder(tf.int16,shape=[None,None],name='feature_label')\n",
    "\n",
    "# feature_size,embedding_size = 40,20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #embeddings\n",
    "# weights = dict()\n",
    "\n",
    "# weights['feature_embeddings'] = tf.Variable(tf.random_normal([feature_size,embedding_size],0.0,0.01),\n",
    "#     name='feature_embeddings')\n",
    "# weights['feature_bias'] = tf.Variable(tf.random_normal([feature_size,1],0.0,1.0),\n",
    "#     name='feature_bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size,field_size,vector_dim = 42,4,1\n",
    "\n",
    "weights0 = tf.Variable(tf.truncated_normal([1]))\n",
    "weights1 = tf.Variable(tf.truncated_normal([input_size]))\n",
    "\n",
    "## input_x_size 表示 one hot code后的变量树\n",
    "## field_size 表示 域数\n",
    "## vector_dim 表示 每个向量v的长度\n",
    "weights2 = tf.Variable(tf.truncated_normal([input_size,field_size,vector_dim]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_x,input_field,weights0,weights1,weights2):\n",
    "    \"\"\"计算回归模型输出的值\"\"\"\n",
    "\n",
    "    secondValue = tf.reduce_sum(tf.multiply(weights1,input_x,name='secondValue')) ### 一次项的值\n",
    "\n",
    "    firstTwoValue = tf.add(weights0, secondValue, name=\"firstTwoValue\")\n",
    "\n",
    "    thirdValue = tf.Variable(0.01,dtype=tf.float32)\n",
    "    input_shape = input_size\n",
    "\n",
    "    # for i in range(input_shape):\n",
    "    #     fieldIndex1 = int(input_field[i])\n",
    "\n",
    "    #     for j in range(i+1,input_shape):\n",
    "    #         fieldIndex2 = int(input_field[j])\n",
    "    #         vectorLeft = tf.convert_to_tensor([[i,j,k] for k in range(vector_dim)])\n",
    "    #         weightLeft = tf.gather_nd(thirdWeight,vectorLeft)\n",
    "    #         weightLeftAfterCut = tf.squeeze(weightLeft)\n",
    "\n",
    "    #         vectorRight = tf.convert_to_tensor([[i,j,k] for k in range(vector_dim)])\n",
    "    #         weightRight = tf.gather_nd(thirdWeight,vectorRight)\n",
    "    #         weightRightAfterCut = tf.squeeze(weightRight)\n",
    "\n",
    "    #         tempValue = tf.reduce_sum(tf.multiply(weightLeftAfterCut,weightRightAfterCut))\n",
    "\n",
    "    #         indices2 = [i]\n",
    "    #         indices3 = [j]\n",
    "\n",
    "    #         xi = tf.squeeze(tf.gather_nd(input_x, indices2))\n",
    "    #         xj = tf.squeeze(tf.gather_nd(input_x, indices3))\n",
    "\n",
    "    #         product = tf.reduce_sum(tf.multiply(xi, xj))\n",
    "\n",
    "    #         secondItemVal = tf.multiply(tempValue, product)\n",
    "\n",
    "    #         tf.assign(thirdValue, tf.add(thirdValue, secondItemVal))\n",
    "\n",
    "    # return tf.add(firstTwoValue,thirdValue)\n",
    "    return firstTwoValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_w = tf.constant(10., name='lambda_w')\n",
    "lambda_v = tf.constant(0.001, name='lambda_v')\n",
    "\n",
    "x_input = tf.placeholder(tf.float32,[None,42])\n",
    "y_input = tf.placeholder(tf.float32,[None,1])\n",
    "\n",
    "y_ = inference(x_input,input_field,weights0,weights1,weights2)\n",
    "\n",
    "# l2_norm = tf.reduce_sum(\n",
    "#     tf.add(\n",
    "#         tf.multiply(lambda_w, tf.pow(oneDimWeights, 2)),\n",
    "#         tf.reduce_sum(tf.multiply(lambda_v, tf.pow(thirdWeight, 2)),axis=[1,2])\n",
    "#     )\n",
    "# )\n",
    "\n",
    "l2_norm = tf.multiply(lambda_w,tf.reduce_sum(tf.pow(weights1,2)))\n",
    "\n",
    "loss = tf.reduce_mean(tf.pow(tf.subtract(y_input,y_),2))\n",
    "loss_ = tf.add(loss,l2_norm)\n",
    "\n",
    "# train_step = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss_)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss_)\n",
    "\n",
    "tf.summary.scalar('loss',loss_)\n",
    "summ_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensorflow.python.framework.ops.Tensor"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# tf.assign_add?\n",
    "# tf.assign_add(loss,l2_norm)\n",
    "type(summ_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\ntrain loss in round 0 : 1958782464\ntest loss in round 0 : 379873472\n10\ntrain loss in round 10 : 1761656448\ntest loss in round 10 : 342726112\n20\ntrain loss in round 20 : 1576742400\ntest loss in round 20 : 307858208\n30\ntrain loss in round 30 : 1405027840\ntest loss in round 30 : 275437312\n40\ntrain loss in round 40 : 1246964352\ntest loss in round 40 : 245549840\n50\ntrain loss in round 50 : 1102409856\ntest loss in round 50 : 218163136\n60\ntrain loss in round 60 : 970919296\ntest loss in round 60 : 193187376\n70\ntrain loss in round 70 : 851851456\ntest loss in round 70 : 170514944\n80\ntrain loss in round 80 : 744483904\ntest loss in round 80 : 150010304\n90\ntrain loss in round 90 : 648052416\ntest loss in round 90 : 131539224\n100\ntrain loss in round 100 : 561823360\ntest loss in round 100 : 114954912\n110\ntrain loss in round 110 : 485037024\ntest loss in round 110 : 100135248\n120\ntrain loss in round 120 : 416979584\ntest loss in round 120 : 86937232\n130\ntrain loss in round 130 : 356897376\ntest loss in round 130 : 75231272\n140\ntrain loss in round 140 : 304108480\ntest loss in round 140 : 64887356\n150\ntrain loss in round 150 : 257974688\ntest loss in round 150 : 55787440\n160\ntrain loss in round 160 : 217810480\ntest loss in round 160 : 47818704\n170\ntrain loss in round 170 : 183054288\ntest loss in round 170 : 40863484\n180\ntrain loss in round 180 : 153110016\ntest loss in round 180 : 34822820\n190\ntrain loss in round 190 : 127439872\ntest loss in round 190 : 29597516\n200\ntrain loss in round 200 : 105568720\ntest loss in round 200 : 25093358\n210\ntrain loss in round 210 : 87007608\ntest loss in round 210 : 21225730\n220\ntrain loss in round 220 : 71366664\ntest loss in round 220 : 17922318\n230\ntrain loss in round 230 : 58232820\ntest loss in round 230 : 15108411\n240\ntrain loss in round 240 : 47282052\ntest loss in round 240 : 12718925\n250\ntrain loss in round 250 : 38196968\ntest loss in round 250 : 10699983\n260\ntrain loss in round 260 : 30699166\ntest loss in round 260 : 9000043\n270\ntrain loss in round 270 : 24545944\ntest loss in round 270 : 7569754\n280\ntrain loss in round 280 : 19526586\ntest loss in round 280 : 6372470\n290\ntrain loss in round 290 : 15452476\ntest loss in round 290 : 5370652\n300\ntrain loss in round 300 : 12163974\ntest loss in round 300 : 4536719\n310\ntrain loss in round 310 : 9528639\ntest loss in round 310 : 3842439\n320\ntrain loss in round 320 : 7423082\ntest loss in round 320 : 3265027\n330\ntrain loss in round 330 : 5752957\ntest loss in round 330 : 2785123\n340\ntrain loss in round 340 : 4436374\ntest loss in round 340 : 2388038\n350\ntrain loss in round 350 : 3401702\ntest loss in round 350 : 2058351\n360\ntrain loss in round 360 : 2596556\ntest loss in round 360 : 1784615\n370\ntrain loss in round 370 : 1971390\ntest loss in round 370 : 1558524\n380\ntrain loss in round 380 : 1488464\ntest loss in round 380 : 1370584\n390\ntrain loss in round 390 : 1117913\ntest loss in round 390 : 1214419\n400\ntrain loss in round 400 : 835391\ntest loss in round 400 : 1085184\n410\ntrain loss in round 410 : 621839\ntest loss in round 410 : 976675\n420\ntrain loss in round 420 : 460183\ntest loss in round 420 : 886764\n430\ntrain loss in round 430 : 338350\ntest loss in round 430 : 812131\n440\ntrain loss in round 440 : 248022\ntest loss in round 440 : 749449\n450\ntrain loss in round 450 : 181128\ntest loss in round 450 : 697091\n460\ntrain loss in round 460 : 131252\ntest loss in round 460 : 653460\n470\ntrain loss in round 470 : 94971\ntest loss in round 470 : 616780\n480\ntrain loss in round 480 : 68634\ntest loss in round 480 : 586214\n490\ntrain loss in round 490 : 49331\ntest loss in round 490 : 560427\n500\ntrain loss in round 500 : 35502\ntest loss in round 500 : 538867\n510\ntrain loss in round 510 : 25581\ntest loss in round 510 : 520778\n520\ntrain loss in round 520 : 18458\ntest loss in round 520 : 505804\n530\ntrain loss in round 530 : 13381\ntest loss in round 530 : 493020\n540\ntrain loss in round 540 : 9888\ntest loss in round 540 : 482588\n550\ntrain loss in round 550 : 7375\ntest loss in round 550 : 473693\n560\ntrain loss in round 560 : 5663\ntest loss in round 560 : 466541\n570\ntrain loss in round 570 : 4425\ntest loss in round 570 : 460150\n580\ntrain loss in round 580 : 3631\ntest loss in round 580 : 455097\n590\ntrain loss in round 590 : 3056\ntest loss in round 590 : 450865\n600\ntrain loss in round 600 : 2671\ntest loss in round 600 : 447212\n610\ntrain loss in round 610 : 2397\ntest loss in round 610 : 444414\n620\ntrain loss in round 620 : 2218\ntest loss in round 620 : 441988\n630\ntrain loss in round 630 : 2115\ntest loss in round 630 : 440278\n640\ntrain loss in round 640 : 2037\ntest loss in round 640 : 438661\n650\ntrain loss in round 650 : 1982\ntest loss in round 650 : 437333\n660\ntrain loss in round 660 : 1950\ntest loss in round 660 : 436403\n670\ntrain loss in round 670 : 1925\ntest loss in round 670 : 435483\n680\ntrain loss in round 680 : 1912\ntest loss in round 680 : 434838\n690\ntrain loss in round 690 : 1902\ntest loss in round 690 : 434255\n700\ntrain loss in round 700 : 1894\ntest loss in round 700 : 433647\n710\ntrain loss in round 710 : 1890\ntest loss in round 710 : 433212\n720\ntrain loss in round 720 : 1888\ntest loss in round 720 : 432967\n730\ntrain loss in round 730 : 1887\ntest loss in round 730 : 432803\n740\ntrain loss in round 740 : 1886\ntest loss in round 740 : 432633\n750\ntrain loss in round 750 : 1885\ntest loss in round 750 : 432497\n760\ntrain loss in round 760 : 1885\ntest loss in round 760 : 432353\n770\ntrain loss in round 770 : 1885\ntest loss in round 770 : 432276\n780\ntrain loss in round 780 : 1884\ntest loss in round 780 : 432111\n790\ntrain loss in round 790 : 1884\ntest loss in round 790 : 432057\n800\ntrain loss in round 800 : 1884\ntest loss in round 800 : 432000\n810\ntrain loss in round 810 : 1884\ntest loss in round 810 : 431987\n820\ntrain loss in round 820 : 1884\ntest loss in round 820 : 431972\n830\ntrain loss in round 830 : 1884\ntest loss in round 830 : 431952\n840\ntrain loss in round 840 : 1884\ntest loss in round 840 : 431945\n850\ntrain loss in round 850 : 1884\ntest loss in round 850 : 431940\n860\ntrain loss in round 860 : 1884\ntest loss in round 860 : 431938\n870\ntrain loss in round 870 : 1884\ntest loss in round 870 : 431934\n880\ntrain loss in round 880 : 1884\ntest loss in round 880 : 431925\n890\ntrain loss in round 890 : 1884\ntest loss in round 890 : 431924\n900\ntrain loss in round 900 : 1884\ntest loss in round 900 : 431918\n910\ntrain loss in round 910 : 1884\ntest loss in round 910 : 431914\n920\ntrain loss in round 920 : 1884\ntest loss in round 920 : 431912\n930\ntrain loss in round 930 : 1884\ntest loss in round 930 : 431908\n940\ntrain loss in round 940 : 1884\ntest loss in round 940 : 431908\n950\ntrain loss in round 950 : 1884\ntest loss in round 950 : 431905\n960\ntrain loss in round 960 : 1884\ntest loss in round 960 : 431901\n970\ntrain loss in round 970 : 1884\ntest loss in round 970 : 431900\n980\ntrain loss in round 980 : 1884\ntest loss in round 980 : 431897\n990\ntrain loss in round 990 : 1884\ntest loss in round 990 : 431897\n"
    }
   ],
   "source": [
    "train_log_path = \"./tmp/train_tmp\"\n",
    "test_log_path = \"./tmp/test_tmp\"\n",
    "\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    summary_writer_tr = tf.summary.FileWriter(train_log_path, graph=sess.graph)\n",
    "    summary_writer_ts = tf.summary.FileWriter(test_log_path)\n",
    "\n",
    "    for i in range(1000):\n",
    "        # sess.run([train_step],feed_dict={x_input:x1.values, y_input:y1})\n",
    "\n",
    "        _,lss_tr,w1,smm = sess.run([train_step,loss_,weights1,summ_op],\n",
    "                                    feed_dict={x_input:X_train.values, y_input:y_train})\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print(i)\n",
    "            summary_writer_tr.add_summary(smm,i)\n",
    "\n",
    "            lss_ts,w1,smm2 = sess.run([loss_,weights1,summ_op],\n",
    "                                    feed_dict={x_input:X_test.values, y_input:y_test})\n",
    "            summary_writer_ts.add_summary(smm2,i)\n",
    "\n",
    "            print('train loss in round %d : %.2d' %(i,round(lss_tr,2)))\n",
    "            print('test loss in round %d : %.2d' %(i,round(lss_ts,2)))\n",
    "\n",
    "    # input_x_batch = x1[1]\n",
    "    # input_y_batch = y1[1]\n",
    "        # print(sess.run(train_step))\n",
    "\n",
    "summary_writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.031287394"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "w1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.5339930270038027"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "reg = Ridge(alpha=10)\n",
    "reg.fit(X_train,y_train)\n",
    "reg.coef_.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'1.8.0'"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'c:\\\\Users\\\\qqmai\\\\Desktop\\\\study\\\\study\\\\deep learning'"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36464bitqqmaivirtualenv362ad2a659b5435fb171141c56735108",
   "display_name": "Python 3.6.4 64-bit ('qqmai': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}