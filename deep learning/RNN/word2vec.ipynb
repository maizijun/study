{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "unable to import 'smart_open.gcs', disabling that module\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# from gensim.test.utils import common_texts,get_tmpfile\n",
    "from gensim.models import word2vec\n",
    "from gensim import corpora\n",
    "\n",
    "\n",
    "from nltk.corpus import reuters,stopwords,brown\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize,RegexpTokenizer\n",
    "# from nltk import Text\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder\n",
    "\n",
    "# from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_ls = pd.DataFrame(reuters.fileids())\n",
    "\n",
    "corp_select = reuters\n",
    "\n",
    "id_ls = pd.DataFrame(corp_select.fileids())\n",
    "categ_ls = pd.DataFrame(corp_select.fileids())\n",
    "\n",
    "text = corp_select.raw(fileids=id_ls[0])\n",
    "# text = brown.raw(categories='humor')\n",
    "sen_ls = sent_tokenize(text)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans=str.maketrans({key: None for key in string.punctuation}) #建立转换关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sen_ls = []\n",
    "word_ls = []\n",
    "reg_tokenizer_b = RegexpTokenizer(r'\\s+',gaps=True) ## 按照空格划分\n",
    "reg_tokenizer_w = RegexpTokenizer(r'\\w+') ## 按照单词\n",
    "\n",
    "for sen in sen_ls[:1000]:\n",
    "    # word_ls.append(word_tokenize(sen))\n",
    "\n",
    "    word_ls_tmp = reg_tokenizer_b.tokenize(sen)\n",
    "    word_ls_tmp = list(filter(lambda x:x not in stopwords.words('english'),word_ls_tmp))\n",
    "    word_ls_tmp = list(filter(lambda x:x not in [*map(lambda x:x.upper(),stopwords.words('english'))],word_ls_tmp))\n",
    "\n",
    "    word_ls_tmp = list(map(lambda x:re.sub(\"[0-9]+\",\"\",x),word_ls_tmp))\n",
    "    word_ls_tmp = list(map(lambda x:x.translate(trans),word_ls_tmp))\n",
    "    word_sen_ls.append(word_ls_tmp)\n",
    "    word_ls = word_ls + word_ls_tmp\n",
    "\n",
    "\n",
    "vob = np.unique(word_ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1000"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "len(word_sen_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(18382, 4742)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "vob_size = len(vob)\n",
    "len(word_ls),len(vob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_x_y(sentence):\n",
    "\n",
    "    window = 4\n",
    "    n = len(sentence)\n",
    "    # print(n)\n",
    "    # if n<=window:\n",
    "    #     pass\n",
    "\n",
    "    x_input = []\n",
    "    y_input = []\n",
    "\n",
    "    for k in range(n-4): #### 遍历句子长度 ####\n",
    "        for w in range(window): #### 对窗口内所有单词组合全部拿出来 ####\n",
    "            x_tmp,y_tmp = sentence[k+w],sentence[k+window]\n",
    "\n",
    "            x_input.append(x_tmp)\n",
    "            y_input.append(y_tmp)\n",
    "\n",
    "    for k in range(n-1,4,-1): #### 遍历句子长度 ####\n",
    "        # print(k)\n",
    "        for w in range(window): #### 对窗口内所有单词组合全部拿出来 ####\n",
    "            x_tmp,y_tmp = sentence[k-w],sentence[k-window]\n",
    "\n",
    "            x_input.append(x_tmp)\n",
    "            y_input.append(y_tmp)\n",
    "\n",
    "        # print(x_tmp,y_tmp)\n",
    "\n",
    "    x_input = pd.DataFrame(x_input)\n",
    "    y_input = pd.DataFrame(y_input)\n",
    "\n",
    "    return x_input,y_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame()\n",
    "y = pd.DataFrame()\n",
    "\n",
    "for sen in word_sen_ls:\n",
    "\n",
    "    if len(sen)<=5:\n",
    "        continue\n",
    "\n",
    "    X_tmp,y_tmp = gen_x_y(sen)\n",
    "\n",
    "    X = pd.concat([X,X_tmp])\n",
    "    y = pd.concat([y,y_tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((111376, 1), (111376, 1))"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "           0\n0      ASIAN\n1  EXPORTERS\n2       FEAR\n3     DAMAGE\n4  EXPORTERS",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ASIAN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>EXPORTERS</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>FEAR</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DAMAGE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>EXPORTERS</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "OrdinalEncoder(categories='auto', dtype=<class 'numpy.float64'>)"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "# ohe = OneHotEncoder()\n",
    "ohe = OrdinalEncoder()\n",
    "ohe.fit(vob.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans = X.apply(lambda x:np.squeeze(ohe.transform(x.reshape(-1,1)),axis=1))\n",
    "y_trans = y.apply(lambda x:np.squeeze(ohe.transform(y[0].reshape(-1,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans = X_trans.astype(int)\n",
    "y_trans = y_trans.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     0\n0   30\n1  484\n2  537\n3  387\n4  484",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>484</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>537</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>387</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>484</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "X_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      0\n0  1611\n1  1611\n2  1611\n3  1611\n4  1274",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1611</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1611</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1611</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1611</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1274</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "y_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RNN CELL ####\n",
    "rnn_mod = 0\n",
    "num_unit = 32\n",
    "\n",
    "if rnn_mod==0:\n",
    "    cell = tf.nn.rnn_cell.BasicRNNCell(num_units=num_unit)\n",
    "if rnn_mod==1:\n",
    "    cell = tf.nn.rnn_cell.RNNCell(num_units=num_unit)\n",
    "if rnn_mod==2:\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(num_units=num_unit)\n",
    "if rnn_mod==3:\n",
    "    cell = tf.nn.rnn_cell.GRUCell(num_units=num_unit)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### word embedding ###\n",
    "embed_size = 100\n",
    "\n",
    "input_x = tf.placeholder(tf.int32,[None])\n",
    "input_y = tf.placeholder(tf.int32,[None,1])\n",
    "\n",
    "word_embeddings = tf.Variable(tf.truncated_normal([vob_size,embed_size],stddev=1,mean=0.1))\n",
    "embedding_word_ids = tf.nn.embedding_lookup(word_embeddings,input_x)\n",
    "\n",
    "weights = tf.Variable(tf.truncated_normal([vob_size,embed_size],mean=1,stddev=1))\n",
    "bias = tf.Variable(tf.truncated_normal([vob_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\qqmai\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n"
    }
   ],
   "source": [
    "loss = tf.reduce_sum(\n",
    "    tf.nn.sampled_softmax_loss(weights=weights,\n",
    "                                biases = bias,\n",
    "                                labels = input_y,\n",
    "                                inputs = embedding_word_ids,\n",
    "                                num_sampled = 20,\n",
    "                                num_classes = vob_size\n",
    "                                ))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 1479593.2\n10 1155886.2\n20 1024167.06\n30 790968.2\n40 569916.25\n50 602854.2\n60 505109.38\n70 401911.94\n80 340491.25\n90 310117.9\n100 340358.7\n110 341198.72\n120 263772.8\n130 292137.4\n140 190056.27\n150 322103.06\n160 278547.88\n170 343993.56\n180 253769.53\n190 214409.47\n200 404969.75\n210 147878.48\n220 200520.56\n230 246971.25\n240 243187.44\n250 212383.38\n260 98627.6\n270 356667.38\n280 379110.3\n290 204087.4\n300 258962.19\n310 113330.484\n320 128491.49\n330 171816.05\n340 104803.125\n350 220149.16\n360 189721.1\n370 139121.47\n380 108038.67\n390 69707.734\n"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(400):\n",
    "        _,losses,wgs = sess.run([optimizer,loss,weights],feed_dict={input_x:np.squeeze(X_trans.values),\n",
    "                                                  input_y:y_trans})\n",
    "\n",
    "        if i%10==0:\n",
    "            print(i,losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(4742, 100)"
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "wgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[4431.]])"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = wgs[int(ohe.transform([['money']])[0,0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector2 = np.dot(wgs,vector1)\n",
    "order2 = vector2.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([['also'],\n       ['would'],\n       ['one'],\n       ['pay'],\n       ['despite'],\n       ['buy'],\n       ['month'],\n       ['Group'],\n       ['including'],\n       ['government']], dtype='<U19')"
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "ohe.inverse_transform(order2[:10,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10\n9\n8\n7\n6\n5\n"
    }
   ],
   "source": [
    "for k in range(10,4,-1):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36464bitqqmaivirtualenv362ad2a659b5435fb171141c56735108",
   "display_name": "Python 3.6.4 64-bit ('qqmai': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}