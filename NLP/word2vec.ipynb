{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts,get_tmpfile\n",
    "from gensim.models import word2vec\n",
    "from gensim import corpora\n",
    "\n",
    "\n",
    "from nltk.corpus import reuters,stopwords\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize,RegexpTokenizer\n",
    "\n",
    "from nltk import Text\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re,string\n",
    "\n",
    "# from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text():\n",
    "    '''        \n",
    "    generate text from corpus\n",
    "    \n",
    "    input: corpus ID\n",
    "    return: list, each for one text\n",
    "    '''\n",
    "\n",
    "    corp_select = reuters\n",
    "    id_ls =  corp_select.fileids()\n",
    "    text_nums = 4000\n",
    "\n",
    "    text_ls = []\n",
    "    for k in range(text_nums):\n",
    "        text_ls.append(corp_select.raw(fileids=id_ls[k]))\n",
    "\n",
    "    return text_ls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sentence(text_ls):\n",
    "    '''\n",
    "    generate tokenized sentences from text list\n",
    "    \n",
    "    input: corpus ID\n",
    "    return: list, each for one sentence\n",
    "    '''\n",
    "\n",
    "    sen_ls = []\n",
    "    for text in text_ls:\n",
    "        sen_ls = sen_ls + sent_tokenize(text)\n",
    "\n",
    "    return sen_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(word):\n",
    "    ''' 去除标点符号 remove punctuation from word'''\n",
    "\n",
    "    trans=str.maketrans({key: None for key in string.punctuation}) #建立转换关系\n",
    "    word = word.translate(trans)\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_word(sentence):\n",
    "    '''\n",
    "    generate tokenized words from sentence\n",
    "    remove punctuation\n",
    "\n",
    "    input: sentence\n",
    "    return: list, each element for one word\n",
    "    '''\n",
    "\n",
    "    word_ls = []\n",
    "    reg_tokenizer_b = RegexpTokenizer(r'\\s+',gaps=True) ## 按照空格划分\n",
    "    reg_tokenizer_w = RegexpTokenizer(r'\\w+') ## 按照单词\n",
    "\n",
    "    \n",
    "        # word_ls.append(word_tokenize(sen))\n",
    "\n",
    "    word_ls = reg_tokenizer_b.tokenize(sentence)\n",
    "    # word_ls = list(filter(lambda x:x not in stopwords.words('english'),word_ls))\n",
    "    # word_ls = list(filter(lambda x:x not in [*map(lambda x:x.upper(),stopwords.words('english'))],word_ls))\n",
    "\n",
    "    word_ls = list(map(lambda x:re.sub(\"[0-9]+\",\"\",x),word_ls))\n",
    "    word_ls = list(map(lambda x:x.lower(),word_ls))\n",
    "        # word_ls_tmp = list(map(lambda x:x.translate(trans),word_ls_tmp))\n",
    "\n",
    "    word_ls = list(map(remove_punc,word_ls))\n",
    "    word_ls = list(filter(lambda x:len(x)>0,word_ls)) ## 去掉空格 \n",
    "\n",
    "    return word_ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(4000, 19520)"
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "text_ls = gen_text()\n",
    "sen_ls = gen_sentence(text_ls)\n",
    "\n",
    "len(text_ls),len(sen_ls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sen_ls = []\n",
    "word_ls = []\n",
    "\n",
    "for xx in sen_ls:\n",
    "    word_sen_ls.append(gen_word(xx))\n",
    "    word_ls = word_ls + gen_word(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4000 19520 468094 20678\n"
    }
   ],
   "source": [
    "vocab = np.unique(word_ls)\n",
    "print(len(text_ls),len(word_sen_ls),len(word_ls),len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ls = pd.Series(word_ls)\n",
    "is_common_word = word_ls.value_counts()>=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1115"
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "source": [
    "vocab = is_common_word[is_common_word==True].index.tolist()\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = word2vec.Word2Vec(word_sen_ls,size=100,window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "6215"
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "model_vocab = list(wv_model.wv.vocab.keys())\n",
    "len(model_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_vec = pd.DataFrame(index=vocab,columns=['vec_'+str(i) for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,word in enumerate(df_word_vec.index[:]):\n",
    "    # if i%100==0:\n",
    "    #     print(i)\n",
    "    df_word_vec.loc[word] = wv_model.wv.word_vec(word).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      vec_0   vec_1   vec_2   vec_3   vec_4   vec_5   vec_6   vec_7   vec_8  \\\nthe -0.1518  0.4571 -0.0052  0.2874 -0.6003 -0.5068  0.4788  0.0543  0.0351   \nof  -0.0841  0.4754  0.5144  0.0162 -0.2329 -0.2922  0.0424 -0.0125  0.3198   \nto   -0.547  0.2666 -0.1222 -0.0656 -1.0751  0.4433  0.4562  1.2213  0.7926   \nin  -0.0899  0.3626  0.0153  0.0658  0.0789 -0.0363  0.1631  0.0857  0.8244   \nand -0.2367  0.0411 -0.3654 -0.3301 -0.5453 -0.0321  0.1703  0.0685  0.2182   \n\n      vec_9   ...    vec_90  vec_91  vec_92  vec_93  vec_94  vec_95  vec_96  \\\nthe  1.3316   ...   -0.0342 -0.4633 -0.2218  1.0928  0.0449 -0.1377  0.5191   \nof   -0.185   ...   -1.0748   0.187  -0.443  0.5509   0.734  0.1633 -0.5504   \nto  -0.1983   ...   -0.4325  0.4254 -0.5346  0.2084  0.4553 -0.6684  0.4974   \nin  -0.3604   ...   -0.8285  0.6386 -0.4927  0.5591  0.0553 -0.0838 -0.7433   \nand  0.0282   ...   -0.4153  0.2055 -0.0755  0.2019  0.0095 -0.2883 -0.8199   \n\n     vec_97  vec_98  vec_99  \nthe -0.0101  0.5749  0.1787  \nof  -0.0323 -0.0636  0.5541  \nto  -0.1434  0.8088 -0.3856  \nin   0.0274  0.3288  0.1136  \nand -0.3857 -0.3366  0.1847  \n\n[5 rows x 100 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vec_0</th>\n      <th>vec_1</th>\n      <th>vec_2</th>\n      <th>vec_3</th>\n      <th>vec_4</th>\n      <th>vec_5</th>\n      <th>vec_6</th>\n      <th>vec_7</th>\n      <th>vec_8</th>\n      <th>vec_9</th>\n      <th>...</th>\n      <th>vec_90</th>\n      <th>vec_91</th>\n      <th>vec_92</th>\n      <th>vec_93</th>\n      <th>vec_94</th>\n      <th>vec_95</th>\n      <th>vec_96</th>\n      <th>vec_97</th>\n      <th>vec_98</th>\n      <th>vec_99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>the</th>\n      <td>-0.1518</td>\n      <td>0.4571</td>\n      <td>-0.0052</td>\n      <td>0.2874</td>\n      <td>-0.6003</td>\n      <td>-0.5068</td>\n      <td>0.4788</td>\n      <td>0.0543</td>\n      <td>0.0351</td>\n      <td>1.3316</td>\n      <td>...</td>\n      <td>-0.0342</td>\n      <td>-0.4633</td>\n      <td>-0.2218</td>\n      <td>1.0928</td>\n      <td>0.0449</td>\n      <td>-0.1377</td>\n      <td>0.5191</td>\n      <td>-0.0101</td>\n      <td>0.5749</td>\n      <td>0.1787</td>\n    </tr>\n    <tr>\n      <th>of</th>\n      <td>-0.0841</td>\n      <td>0.4754</td>\n      <td>0.5144</td>\n      <td>0.0162</td>\n      <td>-0.2329</td>\n      <td>-0.2922</td>\n      <td>0.0424</td>\n      <td>-0.0125</td>\n      <td>0.3198</td>\n      <td>-0.185</td>\n      <td>...</td>\n      <td>-1.0748</td>\n      <td>0.187</td>\n      <td>-0.443</td>\n      <td>0.5509</td>\n      <td>0.734</td>\n      <td>0.1633</td>\n      <td>-0.5504</td>\n      <td>-0.0323</td>\n      <td>-0.0636</td>\n      <td>0.5541</td>\n    </tr>\n    <tr>\n      <th>to</th>\n      <td>-0.547</td>\n      <td>0.2666</td>\n      <td>-0.1222</td>\n      <td>-0.0656</td>\n      <td>-1.0751</td>\n      <td>0.4433</td>\n      <td>0.4562</td>\n      <td>1.2213</td>\n      <td>0.7926</td>\n      <td>-0.1983</td>\n      <td>...</td>\n      <td>-0.4325</td>\n      <td>0.4254</td>\n      <td>-0.5346</td>\n      <td>0.2084</td>\n      <td>0.4553</td>\n      <td>-0.6684</td>\n      <td>0.4974</td>\n      <td>-0.1434</td>\n      <td>0.8088</td>\n      <td>-0.3856</td>\n    </tr>\n    <tr>\n      <th>in</th>\n      <td>-0.0899</td>\n      <td>0.3626</td>\n      <td>0.0153</td>\n      <td>0.0658</td>\n      <td>0.0789</td>\n      <td>-0.0363</td>\n      <td>0.1631</td>\n      <td>0.0857</td>\n      <td>0.8244</td>\n      <td>-0.3604</td>\n      <td>...</td>\n      <td>-0.8285</td>\n      <td>0.6386</td>\n      <td>-0.4927</td>\n      <td>0.5591</td>\n      <td>0.0553</td>\n      <td>-0.0838</td>\n      <td>-0.7433</td>\n      <td>0.0274</td>\n      <td>0.3288</td>\n      <td>0.1136</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>-0.2367</td>\n      <td>0.0411</td>\n      <td>-0.3654</td>\n      <td>-0.3301</td>\n      <td>-0.5453</td>\n      <td>-0.0321</td>\n      <td>0.1703</td>\n      <td>0.0685</td>\n      <td>0.2182</td>\n      <td>0.0282</td>\n      <td>...</td>\n      <td>-0.4153</td>\n      <td>0.2055</td>\n      <td>-0.0755</td>\n      <td>0.2019</td>\n      <td>0.0095</td>\n      <td>-0.2883</td>\n      <td>-0.8199</td>\n      <td>-0.3857</td>\n      <td>-0.3366</td>\n      <td>0.1847</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 100 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 128
    }
   ],
   "source": [
    "df_word_vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([-0.15181644,  0.45714858, -0.00524186,  0.28742513, -0.60031974,\n       -0.5067548 ,  0.47876546,  0.05425365,  0.03506719,  1.3316201 ,\n        0.76646566, -0.6048728 , -0.5493429 , -0.3991539 ,  0.94563085,\n        0.22750299,  0.412729  , -0.64330274, -0.29419848, -1.1044321 ,\n        0.3881118 ,  0.66417676, -0.44369525, -0.9205933 ,  0.40129432,\n       -1.0497384 ,  0.23863624,  1.1829684 ,  0.19989215,  0.00802428,\n        0.33290794,  0.24115509,  0.48210523,  0.45008874, -1.0136256 ,\n        0.25461698,  0.47712317, -0.27585095, -0.39312762,  0.7589427 ,\n        0.16635601,  0.03966455,  0.0150586 , -0.11332312, -1.6524109 ,\n       -0.41302818,  0.43863103, -1.2761809 , -0.06216665, -0.22265533,\n       -0.7991344 , -0.17343867,  0.5568801 , -0.5604664 ,  0.4138363 ,\n        0.24328394,  0.09199163,  0.25972098, -1.1394298 ,  0.7562514 ,\n        0.6668302 ,  0.18704392, -0.4377473 , -0.09302838,  0.1436386 ,\n        0.13761836, -0.26423758, -0.3485164 ,  0.27410793,  0.72716594,\n        0.13579369, -0.08731052,  0.03605252,  0.08603851,  0.20265996,\n       -0.99697196,  1.040914  , -0.08910931,  0.44358596, -0.7489517 ,\n       -0.591498  ,  0.24772115,  0.09231891, -0.1333097 ,  0.14103365,\n       -0.6363689 ,  0.42328387, -0.24759014, -0.02148809,  0.18156473,\n       -0.03420592, -0.4633064 , -0.22181359,  1.0928295 ,  0.0448721 ,\n       -0.1377008 ,  0.5190566 , -0.01013827,  0.5748806 ,  0.17870817],\n      dtype=float32)"
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "wv_model.wv.word_vec('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Hello Mr. Smith, how are you doing today?',\n 'The weather is great, and Python is awesome.',\n 'The sky is pinkish-blue.',\n \"You shouldn't eat cardboard.\"]"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "sent_tokenize(EXAMPLE_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "FileSystemPathPointer('C:\\\\Users\\\\qqmai\\\\nltk_data')"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "getstream() missing 1 required positional argument: 'self'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-4fdea2bf1677>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextCorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: getstream() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "corpora.textcorpus.TextCorpus.getstream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36464bitqqmaivirtualenv362ad2a659b5435fb171141c56735108",
   "display_name": "Python 3.6.4 64-bit ('qqmai': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}